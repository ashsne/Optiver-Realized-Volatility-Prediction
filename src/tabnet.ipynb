{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-19T20:16:52.569432Z",
     "iopub.status.busy": "2021-08-19T20:16:52.568516Z",
     "iopub.status.idle": "2021-08-19T20:16:52.583270Z",
     "shell.execute_reply": "2021-08-19T20:16:52.582135Z",
     "shell.execute_reply.started": "2021-08-19T20:16:52.569262Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('max_rows', 300)\n",
    "pd.set_option('max_columns', 300)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "def nans(df): return df[df.isnull().any(axis=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T20:16:52.586302Z",
     "iopub.status.busy": "2021-08-19T20:16:52.585440Z",
     "iopub.status.idle": "2021-08-19T20:16:52.598829Z",
     "shell.execute_reply": "2021-08-19T20:16:52.597300Z",
     "shell.execute_reply.started": "2021-08-19T20:16:52.586241Z"
    }
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = \"/home/ash/Documents/Projects/kaggle/ORVP/data/\"\n",
    "# DATA_DIR = \"/home/ash/Documents/Projects/kaggle/ORVP/data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T20:16:52.601622Z",
     "iopub.status.busy": "2021-08-19T20:16:52.601207Z",
     "iopub.status.idle": "2021-08-19T20:16:52.621248Z",
     "shell.execute_reply": "2021-08-19T20:16:52.619709Z",
     "shell.execute_reply.started": "2021-08-19T20:16:52.601583Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def import_data(file, ftype=\"csv\"):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    if ftype==\"csv\":\n",
    "        df = pd.read_csv(file)\n",
    "    if ftype==\"parquet\":\n",
    "        df = pd.read_parquet(file)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T20:16:52.623402Z",
     "iopub.status.busy": "2021-08-19T20:16:52.623045Z",
     "iopub.status.idle": "2021-08-19T20:16:52.639188Z",
     "shell.execute_reply": "2021-08-19T20:16:52.637954Z",
     "shell.execute_reply.started": "2021-08-19T20:16:52.623368Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff() \n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function for preprocessing book data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T20:16:52.641588Z",
     "iopub.status.busy": "2021-08-19T20:16:52.640966Z",
     "iopub.status.idle": "2021-08-19T20:16:52.660626Z",
     "shell.execute_reply": "2021-08-19T20:16:52.659333Z",
     "shell.execute_reply.started": "2021-08-19T20:16:52.641545Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor_book(file_path):\n",
    "    df = import_data(file_path, \"parquet\")\n",
    "    #calculate return etc\n",
    "    df['wap'] = calc_wap1(df)\n",
    "    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n",
    "    \n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n",
    "    \n",
    "    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n",
    "    \n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "\n",
    "    #dict for aggregate\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'log_return2':[realized_volatility],\n",
    "        'wap_balance':[np.mean],\n",
    "        'price_spread':[np.mean],\n",
    "        'bid_spread':[np.mean],\n",
    "        'ask_spread':[np.mean],\n",
    "        'volume_imbalance':[np.mean],\n",
    "        'total_volume':[np.mean],\n",
    "        'wap':[np.mean],\n",
    "            }\n",
    "\n",
    "    #####groupby / all seconds\n",
    "    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "    \n",
    "    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n",
    "        \n",
    "    ######groupby / last XX seconds\n",
    "    last_seconds = [450, 300, 150]\n",
    "    \n",
    "    for second in last_seconds:\n",
    "        second = 600 - second \n",
    "    \n",
    "        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "\n",
    "        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n",
    "     \n",
    "        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n",
    "\n",
    "        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n",
    "        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n",
    "    \n",
    "    #create row_id\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature = df_feature.drop(['time_id_'],axis=1)\n",
    "    \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function for preprocessing trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T20:16:52.662465Z",
     "iopub.status.busy": "2021-08-19T20:16:52.661980Z",
     "iopub.status.idle": "2021-08-19T20:16:52.680042Z",
     "shell.execute_reply": "2021-08-19T20:16:52.679076Z",
     "shell.execute_reply.started": "2021-08-19T20:16:52.662415Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor_trade(file_path):\n",
    "    df = import_data(file_path, \"parquet\")\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    aggregate_dictionary = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "    \n",
    "    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n",
    "    \n",
    "    df_feature = df_feature.reset_index()\n",
    "    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "\n",
    "    \n",
    "    ######groupby / last XX seconds\n",
    "    last_seconds = [450, 300, 150]\n",
    "    \n",
    "    for second in last_seconds:\n",
    "        second = 600 - second\n",
    "    \n",
    "        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n",
    "        df_feature_sec = df_feature_sec.reset_index()\n",
    "        \n",
    "        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n",
    "        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n",
    "        \n",
    "        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n",
    "        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n",
    "    \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined preprocessor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T20:16:52.708965Z",
     "iopub.status.busy": "2021-08-19T20:16:52.708331Z",
     "iopub.status.idle": "2021-08-19T20:16:52.717346Z",
     "shell.execute_reply": "2021-08-19T20:16:52.716115Z",
     "shell.execute_reply.started": "2021-08-19T20:16:52.708926Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    from joblib import Parallel, delayed # parallel computing to save time\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "            \n",
    "        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n",
    "     \n",
    "        return pd.concat([df,df_tmp])\n",
    "    \n",
    "    df = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n",
    "        )\n",
    "\n",
    "    df =  pd.concat(df,ignore_index = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_dir + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train.stock_id.unique()\n",
    "df_train = preprocessor(list_stock_ids= train_ids, is_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_original = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T20:16:52.719576Z",
     "iopub.status.busy": "2021-08-19T20:16:52.719201Z"
    }
   },
   "outputs": [],
   "source": [
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "train = train[['row_id','target']]\n",
    "df_train = train.merge(df_train, on = ['row_id'], how = 'left')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "test_ids = test.stock_id.unique()\n",
    "df_test = preprocessor(list_stock_ids= test_ids, is_train = False)\n",
    "df_test = test.merge(df_test, on = ['row_id'], how = 'left')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding by stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "#stock_id target encoding\n",
    "df_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\n",
    "df_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n",
    "\n",
    "stock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \n",
    "df_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n",
    "\n",
    "#training\n",
    "tmp = np.repeat(np.nan, df_train.shape[0])\n",
    "kf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\n",
    "for idx_1, idx_2 in kf.split(df_train):\n",
    "    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n",
    "\n",
    "    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\n",
    "df_train['stock_id_target_enc'] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['stock_id'] = df_train['stock_id'].astype(int)\n",
    "df_test['stock_id'] = df_test['stock_id'].astype(int)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna(df_train.mean(), inplace=True)\n",
    "\n",
    "df_train.to_pickle(\"df_train.pkl\")\n",
    "df_test.to_pickle(\"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('max_rows', 300)\n",
    "pd.set_option('max_columns', 300)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "def nans(df): return df[df.isnull().any(axis=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SEED = 42\n",
    "\n",
    "def random_seed(SEED):\n",
    "    \n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "class RMSPEMetric(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return rmspe(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample: 428932\n",
      "testing sample: 3\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_pickle(\"df_train.pkl\")\n",
    "df_test = pd.read_pickle(\"df_test.pkl\")\n",
    "print(\"training sample: \" + str(len(df_train)))\n",
    "print(\"testing sample: \" + str(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the non-categorical features\n",
    "scales = df_train.drop(['row_id', 'target', 'stock_id'], axis = 1).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[scales])\n",
    "\n",
    "# Save the Scaler model\n",
    "scaler_name = \"scaler\"\n",
    "pickle.dump(scaler, open(scaler_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical labels\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "le=LabelEncoder()\n",
    "le.fit(df_train[\"stock_id\"])\n",
    "df_train[\"stock_id\"] = le.transform(df_train[\"stock_id\"])\n",
    "\n",
    "with open( 'stock_id_encoder.txt', 'wb') as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, random_state=19901028, shuffle=True)\n",
    "models = []                          # models\n",
    "scores = 0.0                         # validation score\n",
    "bestscores=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_params = dict(\n",
    "    n_d = 16,\n",
    "    n_a = 16,\n",
    "    n_steps = 3,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = 1e-2, weight_decay = 1e-5),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = 42,\n",
    "    #verbose = 5,\n",
    "    cat_dims=[len(le.classes_)], cat_emb_dim=[10], cat_idxs=[-1] # define categorical features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n",
      "Fold : 1\n",
      "training samples: 343145\n",
      "validation samples: 85787\n",
      "--------Start training--------\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.05464 | train_rmspe: 1.35422 | valid_rmspe: 1.27686 |  0:00:09s\n",
      "epoch 1  | loss: 0.00161 | train_rmspe: 0.85748 | valid_rmspe: 0.96829 |  0:00:17s\n",
      "epoch 2  | loss: 0.00123 | train_rmspe: 0.63591 | valid_rmspe: 0.52873 |  0:00:26s\n",
      "epoch 3  | loss: 0.00105 | train_rmspe: 0.52356 | valid_rmspe: 0.40656 |  0:00:35s\n",
      "epoch 4  | loss: 0.001   | train_rmspe: 0.82277 | valid_rmspe: 0.41624 |  0:00:44s\n",
      "epoch 5  | loss: 0.00096 | train_rmspe: 0.51095 | valid_rmspe: 0.37339 |  0:00:53s\n",
      "epoch 6  | loss: 0.00093 | train_rmspe: 0.37975 | valid_rmspe: 0.35702 |  0:01:02s\n",
      "epoch 7  | loss: 0.00091 | train_rmspe: 0.46656 | valid_rmspe: 0.44007 |  0:01:11s\n",
      "epoch 8  | loss: 0.00089 | train_rmspe: 0.36905 | valid_rmspe: 0.33272 |  0:01:20s\n",
      "epoch 9  | loss: 0.00088 | train_rmspe: 0.60255 | valid_rmspe: 0.33541 |  0:01:29s\n",
      "epoch 10 | loss: 0.00086 | train_rmspe: 0.40212 | valid_rmspe: 0.37362 |  0:01:38s\n",
      "epoch 11 | loss: 0.00086 | train_rmspe: 0.31718 | valid_rmspe: 0.33108 |  0:01:47s\n",
      "epoch 12 | loss: 0.00084 | train_rmspe: 0.31822 | valid_rmspe: 0.32469 |  0:01:56s\n",
      "epoch 13 | loss: 0.00083 | train_rmspe: 0.32351 | valid_rmspe: 0.29647 |  0:02:06s\n",
      "epoch 14 | loss: 0.00083 | train_rmspe: 0.31308 | valid_rmspe: 0.29993 |  0:02:15s\n",
      "epoch 15 | loss: 0.00083 | train_rmspe: 0.3185  | valid_rmspe: 0.30379 |  0:02:25s\n",
      "epoch 16 | loss: 0.00082 | train_rmspe: 0.3737  | valid_rmspe: 0.41318 |  0:02:35s\n",
      "epoch 17 | loss: 0.00081 | train_rmspe: 0.34265 | valid_rmspe: 0.32046 |  0:02:45s\n",
      "epoch 18 | loss: 0.00081 | train_rmspe: 0.3034  | valid_rmspe: 0.29189 |  0:02:54s\n",
      "epoch 19 | loss: 0.00081 | train_rmspe: 0.34543 | valid_rmspe: 0.33557 |  0:03:04s\n",
      "epoch 20 | loss: 0.00081 | train_rmspe: 0.35878 | valid_rmspe: 0.31867 |  0:03:14s\n",
      "epoch 21 | loss: 0.0008  | train_rmspe: 0.32056 | valid_rmspe: 0.30275 |  0:03:24s\n",
      "epoch 22 | loss: 0.0008  | train_rmspe: 0.30613 | valid_rmspe: 0.29907 |  0:03:34s\n",
      "epoch 23 | loss: 0.0008  | train_rmspe: 0.28815 | valid_rmspe: 0.28334 |  0:03:44s\n",
      "epoch 24 | loss: 0.0008  | train_rmspe: 0.30055 | valid_rmspe: 0.30144 |  0:03:56s\n",
      "epoch 25 | loss: 0.00079 | train_rmspe: 0.29461 | valid_rmspe: 0.28165 |  0:04:06s\n",
      "epoch 26 | loss: 0.00079 | train_rmspe: 0.31093 | valid_rmspe: 0.30302 |  0:04:16s\n",
      "epoch 27 | loss: 0.00078 | train_rmspe: 0.32721 | valid_rmspe: 0.30464 |  0:04:26s\n",
      "epoch 28 | loss: 0.00079 | train_rmspe: 0.31217 | valid_rmspe: 0.29296 |  0:04:36s\n",
      "epoch 29 | loss: 0.00078 | train_rmspe: 0.31555 | valid_rmspe: 0.30246 |  0:04:45s\n",
      "epoch 30 | loss: 0.00078 | train_rmspe: 0.26383 | valid_rmspe: 0.25982 |  0:04:54s\n",
      "epoch 31 | loss: 0.00079 | train_rmspe: 0.2744  | valid_rmspe: 0.26043 |  0:05:04s\n",
      "epoch 32 | loss: 0.00078 | train_rmspe: 0.268   | valid_rmspe: 0.25936 |  0:05:13s\n",
      "epoch 33 | loss: 0.00079 | train_rmspe: 0.28804 | valid_rmspe: 0.27504 |  0:05:23s\n",
      "epoch 34 | loss: 0.00079 | train_rmspe: 0.30462 | valid_rmspe: 0.28255 |  0:05:33s\n",
      "epoch 35 | loss: 0.00078 | train_rmspe: 0.29894 | valid_rmspe: 0.28365 |  0:05:43s\n",
      "epoch 36 | loss: 0.00078 | train_rmspe: 0.35228 | valid_rmspe: 0.33367 |  0:05:53s\n",
      "epoch 37 | loss: 0.00078 | train_rmspe: 0.28773 | valid_rmspe: 0.27822 |  0:06:03s\n",
      "epoch 38 | loss: 0.00078 | train_rmspe: 0.32318 | valid_rmspe: 0.3136  |  0:06:13s\n",
      "epoch 39 | loss: 0.00078 | train_rmspe: 0.30638 | valid_rmspe: 0.32544 |  0:06:23s\n",
      "epoch 40 | loss: 0.00078 | train_rmspe: 0.32555 | valid_rmspe: 0.29951 |  0:06:32s\n",
      "epoch 41 | loss: 0.00078 | train_rmspe: 0.27046 | valid_rmspe: 0.26734 |  0:06:42s\n",
      "epoch 42 | loss: 0.00078 | train_rmspe: 0.35189 | valid_rmspe: 0.34133 |  0:06:51s\n",
      "epoch 43 | loss: 0.00077 | train_rmspe: 0.32847 | valid_rmspe: 0.32544 |  0:07:01s\n",
      "epoch 44 | loss: 0.00082 | train_rmspe: 0.29599 | valid_rmspe: 0.28317 |  0:07:11s\n",
      "epoch 45 | loss: 0.00077 | train_rmspe: 0.31201 | valid_rmspe: 0.29423 |  0:07:22s\n",
      "epoch 46 | loss: 0.00077 | train_rmspe: 0.2864  | valid_rmspe: 0.27693 |  0:07:34s\n",
      "epoch 47 | loss: 0.00078 | train_rmspe: 0.26193 | valid_rmspe: 0.25778 |  0:07:47s\n",
      "epoch 48 | loss: 0.00077 | train_rmspe: 0.31392 | valid_rmspe: 0.29748 |  0:07:59s\n",
      "epoch 49 | loss: 0.00077 | train_rmspe: 0.27725 | valid_rmspe: 0.26853 |  0:08:11s\n",
      "epoch 50 | loss: 0.00077 | train_rmspe: 0.29334 | valid_rmspe: 0.27822 |  0:08:23s\n",
      "epoch 51 | loss: 0.00077 | train_rmspe: 0.33451 | valid_rmspe: 0.32218 |  0:08:33s\n",
      "epoch 52 | loss: 0.00078 | train_rmspe: 0.28457 | valid_rmspe: 0.27258 |  0:08:44s\n",
      "epoch 53 | loss: 0.00077 | train_rmspe: 0.29134 | valid_rmspe: 0.28564 |  0:08:54s\n",
      "epoch 54 | loss: 0.00077 | train_rmspe: 0.29301 | valid_rmspe: 0.28208 |  0:09:04s\n",
      "epoch 55 | loss: 0.00077 | train_rmspe: 0.27671 | valid_rmspe: 0.26593 |  0:09:13s\n",
      "epoch 56 | loss: 0.00076 | train_rmspe: 0.28161 | valid_rmspe: 0.27261 |  0:09:23s\n",
      "epoch 57 | loss: 0.00077 | train_rmspe: 0.27043 | valid_rmspe: 0.25969 |  0:09:33s\n",
      "epoch 58 | loss: 0.00077 | train_rmspe: 0.29552 | valid_rmspe: 0.28459 |  0:09:43s\n",
      "epoch 59 | loss: 0.00077 | train_rmspe: 0.27168 | valid_rmspe: 0.25988 |  0:09:52s\n",
      "epoch 60 | loss: 0.00077 | train_rmspe: 0.26883 | valid_rmspe: 0.26111 |  0:10:02s\n",
      "epoch 61 | loss: 0.00077 | train_rmspe: 0.28041 | valid_rmspe: 0.27115 |  0:10:11s\n",
      "epoch 62 | loss: 0.00077 | train_rmspe: 0.28892 | valid_rmspe: 0.27559 |  0:10:21s\n",
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 47 and best_valid_rmspe = 0.25778\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_model_test_0.zip\n",
      "Performance of the　prediction: , RMSPE: 0.2578\n",
      "****************************************************************************************************\n",
      "Fold : 2\n",
      "training samples: 343145\n",
      "validation samples: 85787\n",
      "--------Start training--------\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.05442 | train_rmspe: 3.34452 | valid_rmspe: 1.94909 |  0:00:09s\n",
      "epoch 1  | loss: 0.00167 | train_rmspe: 3.58354 | valid_rmspe: 2.6418  |  0:00:19s\n",
      "epoch 2  | loss: 0.00134 | train_rmspe: 1.09385 | valid_rmspe: 0.94953 |  0:00:29s\n",
      "epoch 3  | loss: 0.00109 | train_rmspe: 0.71813 | valid_rmspe: 0.53431 |  0:00:39s\n",
      "epoch 4  | loss: 0.00103 | train_rmspe: 0.41585 | valid_rmspe: 0.38929 |  0:00:49s\n",
      "epoch 5  | loss: 0.00097 | train_rmspe: 0.42653 | valid_rmspe: 0.41369 |  0:00:59s\n",
      "epoch 6  | loss: 0.00093 | train_rmspe: 0.34893 | valid_rmspe: 0.33607 |  0:01:08s\n",
      "epoch 7  | loss: 0.00091 | train_rmspe: 0.34675 | valid_rmspe: 0.32785 |  0:01:18s\n",
      "epoch 8  | loss: 0.00088 | train_rmspe: 0.32395 | valid_rmspe: 0.3125  |  0:01:28s\n",
      "epoch 9  | loss: 0.00086 | train_rmspe: 0.32847 | valid_rmspe: 0.31585 |  0:01:38s\n",
      "epoch 10 | loss: 0.00085 | train_rmspe: 0.32059 | valid_rmspe: 0.3129  |  0:01:48s\n",
      "epoch 11 | loss: 0.00085 | train_rmspe: 0.32235 | valid_rmspe: 0.30018 |  0:01:58s\n",
      "epoch 12 | loss: 0.00084 | train_rmspe: 0.34777 | valid_rmspe: 0.3095  |  0:02:07s\n",
      "epoch 13 | loss: 0.00083 | train_rmspe: 0.32272 | valid_rmspe: 0.30688 |  0:02:17s\n",
      "epoch 14 | loss: 0.00082 | train_rmspe: 0.32628 | valid_rmspe: 0.30768 |  0:02:27s\n",
      "epoch 15 | loss: 0.00082 | train_rmspe: 0.33332 | valid_rmspe: 0.31672 |  0:02:37s\n",
      "epoch 16 | loss: 0.00082 | train_rmspe: 0.32259 | valid_rmspe: 0.30514 |  0:02:47s\n",
      "epoch 17 | loss: 0.00081 | train_rmspe: 0.30533 | valid_rmspe: 0.28896 |  0:02:56s\n",
      "epoch 18 | loss: 0.00081 | train_rmspe: 0.32307 | valid_rmspe: 0.31329 |  0:03:06s\n",
      "epoch 19 | loss: 0.00081 | train_rmspe: 0.38812 | valid_rmspe: 0.35537 |  0:03:15s\n",
      "epoch 20 | loss: 0.00081 | train_rmspe: 0.35311 | valid_rmspe: 0.34301 |  0:03:24s\n",
      "epoch 21 | loss: 0.0008  | train_rmspe: 0.32905 | valid_rmspe: 0.31636 |  0:03:33s\n",
      "epoch 22 | loss: 0.00079 | train_rmspe: 0.28984 | valid_rmspe: 0.27811 |  0:03:43s\n",
      "epoch 23 | loss: 0.0008  | train_rmspe: 0.40044 | valid_rmspe: 0.38352 |  0:03:52s\n",
      "epoch 24 | loss: 0.00079 | train_rmspe: 0.28343 | valid_rmspe: 0.27276 |  0:04:02s\n",
      "epoch 25 | loss: 0.00079 | train_rmspe: 0.33703 | valid_rmspe: 0.27166 |  0:04:11s\n",
      "epoch 26 | loss: 0.00079 | train_rmspe: 0.33226 | valid_rmspe: 0.30656 |  0:04:21s\n",
      "epoch 27 | loss: 0.00079 | train_rmspe: 0.29956 | valid_rmspe: 0.28622 |  0:04:30s\n",
      "epoch 28 | loss: 0.00079 | train_rmspe: 0.29452 | valid_rmspe: 0.28011 |  0:04:39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 | loss: 0.00078 | train_rmspe: 0.29546 | valid_rmspe: 0.27683 |  0:04:48s\n",
      "epoch 30 | loss: 0.00078 | train_rmspe: 0.31263 | valid_rmspe: 0.30216 |  0:04:59s\n",
      "epoch 31 | loss: 0.00078 | train_rmspe: 0.3182  | valid_rmspe: 0.2974  |  0:05:08s\n",
      "epoch 32 | loss: 0.00078 | train_rmspe: 0.29461 | valid_rmspe: 0.28062 |  0:05:17s\n",
      "epoch 33 | loss: 0.00078 | train_rmspe: 0.30771 | valid_rmspe: 0.29508 |  0:05:26s\n",
      "epoch 34 | loss: 0.00078 | train_rmspe: 0.28449 | valid_rmspe: 0.27153 |  0:05:35s\n",
      "epoch 35 | loss: 0.00077 | train_rmspe: 0.34693 | valid_rmspe: 0.33273 |  0:05:44s\n",
      "epoch 36 | loss: 0.00077 | train_rmspe: 0.33278 | valid_rmspe: 0.30024 |  0:05:53s\n",
      "epoch 37 | loss: 0.00077 | train_rmspe: 0.34102 | valid_rmspe: 0.32496 |  0:06:03s\n",
      "epoch 38 | loss: 0.00078 | train_rmspe: 0.28459 | valid_rmspe: 0.2718  |  0:06:12s\n",
      "epoch 39 | loss: 0.00077 | train_rmspe: 0.2885  | valid_rmspe: 0.27132 |  0:06:21s\n",
      "epoch 40 | loss: 0.00077 | train_rmspe: 0.30726 | valid_rmspe: 0.29586 |  0:06:30s\n",
      "epoch 41 | loss: 0.00077 | train_rmspe: 0.28396 | valid_rmspe: 0.26886 |  0:06:39s\n",
      "epoch 42 | loss: 0.00077 | train_rmspe: 0.28929 | valid_rmspe: 0.27689 |  0:06:48s\n",
      "epoch 43 | loss: 0.00077 | train_rmspe: 0.32973 | valid_rmspe: 0.31034 |  0:06:57s\n",
      "epoch 44 | loss: 0.00077 | train_rmspe: 0.34538 | valid_rmspe: 0.33511 |  0:07:06s\n",
      "epoch 45 | loss: 0.00077 | train_rmspe: 0.31077 | valid_rmspe: 0.30098 |  0:07:16s\n",
      "epoch 46 | loss: 0.00078 | train_rmspe: 0.28742 | valid_rmspe: 0.28009 |  0:07:25s\n",
      "epoch 47 | loss: 0.00078 | train_rmspe: 0.43169 | valid_rmspe: 0.4192  |  0:07:34s\n",
      "epoch 48 | loss: 0.00077 | train_rmspe: 0.2773  | valid_rmspe: 0.26231 |  0:07:43s\n",
      "epoch 49 | loss: 0.00077 | train_rmspe: 0.28654 | valid_rmspe: 0.27525 |  0:07:52s\n",
      "epoch 50 | loss: 0.00078 | train_rmspe: 0.29521 | valid_rmspe: 0.27838 |  0:08:02s\n",
      "epoch 51 | loss: 0.00077 | train_rmspe: 0.36104 | valid_rmspe: 0.3455  |  0:08:11s\n",
      "epoch 52 | loss: 0.00078 | train_rmspe: 0.29616 | valid_rmspe: 0.27749 |  0:08:20s\n",
      "epoch 53 | loss: 0.00077 | train_rmspe: 0.29044 | valid_rmspe: 0.27887 |  0:08:30s\n",
      "epoch 54 | loss: 0.00077 | train_rmspe: 0.29508 | valid_rmspe: 0.27902 |  0:08:39s\n",
      "epoch 55 | loss: 0.00077 | train_rmspe: 0.28594 | valid_rmspe: 0.26982 |  0:08:49s\n",
      "epoch 56 | loss: 0.00078 | train_rmspe: 0.33231 | valid_rmspe: 0.3173  |  0:08:59s\n",
      "epoch 57 | loss: 0.00077 | train_rmspe: 0.29854 | valid_rmspe: 0.28634 |  0:09:08s\n",
      "epoch 58 | loss: 0.00077 | train_rmspe: 0.28361 | valid_rmspe: 0.26912 |  0:09:17s\n",
      "epoch 59 | loss: 0.00078 | train_rmspe: 0.53226 | valid_rmspe: 0.5174  |  0:09:25s\n",
      "epoch 60 | loss: 0.00077 | train_rmspe: 0.36194 | valid_rmspe: 0.34326 |  0:09:34s\n",
      "epoch 61 | loss: 0.00077 | train_rmspe: 0.29553 | valid_rmspe: 0.27788 |  0:09:43s\n",
      "epoch 62 | loss: 0.00077 | train_rmspe: 0.28721 | valid_rmspe: 0.27486 |  0:09:53s\n",
      "epoch 63 | loss: 0.00077 | train_rmspe: 0.31942 | valid_rmspe: 0.30581 |  0:10:02s\n",
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 48 and best_valid_rmspe = 0.26231\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_model_test_1.zip\n",
      "Performance of the　prediction: , RMSPE: 0.2623\n",
      "****************************************************************************************************\n",
      "Fold : 3\n",
      "training samples: 343146\n",
      "validation samples: 85786\n",
      "--------Start training--------\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.057   | train_rmspe: 2.43244 | valid_rmspe: 2.40997 |  0:00:08s\n",
      "epoch 1  | loss: 0.00184 | train_rmspe: 0.88748 | valid_rmspe: 0.83118 |  0:00:17s\n",
      "epoch 2  | loss: 0.00123 | train_rmspe: 0.49284 | valid_rmspe: 0.45203 |  0:00:26s\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(df_train)):\n",
    "\n",
    "    print(\"Fold :\", fold+1)\n",
    "    \n",
    "    # create dataset\n",
    "    traindf, validdf = df_train.loc[trn_idx], df_train.loc[val_idx]\n",
    "    \n",
    "    traindf = traindf.reset_index(drop=True)\n",
    "    validdf = validdf.reset_index(drop=True)\n",
    "    \n",
    "    ## Normalization except stock id ; stock id is used as categoral features\n",
    "\n",
    "    X_train = traindf.drop([\"row_id\",\"target\",\"stock_id\"], axis = 1).values\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_traindf = pd.DataFrame(X_train) \n",
    "\n",
    "    X_traindf[\"stock_id\"] = traindf[\"stock_id\"]\n",
    "\n",
    "\n",
    "    X_train = X_traindf.values\n",
    "    y_train = traindf[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "    # validation is same\n",
    "    X_valid = validdf.drop([\"row_id\",\"target\",\"stock_id\"], axis = 1).values\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "\n",
    "    X_validdf = pd.DataFrame(X_valid)\n",
    "\n",
    "    X_validdf[\"stock_id\"]=validdf[\"stock_id\"]\n",
    "\n",
    "    X_valid = X_validdf.values\n",
    "    y_valid = validdf[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "    # calculate weight\n",
    "\n",
    "    y_weight = 1/np.square(y_train)\n",
    "    \n",
    "    print(\"training samples: \" + str(len(y_train)))\n",
    "    print(\"validation samples: \" + str(len(y_valid)))\n",
    "\n",
    "    # initialize random seed\n",
    "\n",
    "    random_seed(SEED)\n",
    "    \n",
    "    print(\"--------Start training--------\")\n",
    "    \n",
    "    # model \n",
    "    \n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "    model.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "        eval_name=['train', 'valid'],\n",
    "#         eval_metric=['rmse'],\n",
    "        eval_metric=[RMSPEMetric],\n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        batch_size=1024*2, virtual_batch_size=128*2,\n",
    "        num_workers=4,\n",
    "        drop_last=False,\n",
    "#         weights = y_weight,\n",
    "        loss_fn=nn.L1Loss(),\n",
    "    )\n",
    "    \n",
    "    # save tabnet model\n",
    "    saving_path_name = \"tabnet_model_test_\" + str(fold)\n",
    "    saved_filepath = model.save_model(saving_path_name)\n",
    "    bestscores.append(model.best_cost)\n",
    "    \n",
    "    \n",
    "    # validation \n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),4)\n",
    "    print(f'Performance of the　prediction: , RMSPE: {RMSPE}')\n",
    "\n",
    "    #keep scores and models\n",
    "    scores += RMSPE / 5\n",
    "    models.append(model)\n",
    "    print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## fillna for test data ##\n",
    "\n",
    "df_test = pd.read_pickle(\"df_test.pkl\")\n",
    "\n",
    "for col in df_test.columns.to_list()[3:]:\n",
    "    df_test[col] = df_test[col].fillna(df_train[col].mean())\n",
    "\n",
    "\n",
    "# ### normarize ###    \n",
    "\n",
    "x_test = df_test.drop(['row_id', 'time_id',\"stock_id\"], axis = 1).values\n",
    "    # Transform stock id to a numeric value\n",
    "\n",
    "x_test = scaler.transform(x_test)\n",
    "X_testdf = pd.DataFrame(x_test)\n",
    "X_testdf[\"stock_id\"]=df_test[\"stock_id\"]\n",
    "\n",
    "# Label encoding\n",
    "X_testdf[\"stock_id\"] = le.transform(X_testdf[\"stock_id\"])\n",
    "x_test = X_testdf.values\n",
    "\n",
    "target = np.zeros(len(X_test))\n",
    "\n",
    "for model in models:\n",
    "    target = model.predict(x_test)\n",
    "    target += pred / len(models)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df_test[['row_id']]\n",
    "y_pred = y_pred.assign(target = target)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import os\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5adfc325e83fd055f804ac077b7e899434fe915745e765d870a753eed125059e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
